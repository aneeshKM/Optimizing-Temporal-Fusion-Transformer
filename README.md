# âš¡ Optimizing Transformerâ€“Based Time Series Forecasting (TFT)

> **High Performance Machine Learning (HPML) â€“ Spring 2025 Project**  
> **Team:** Aneesh Mokashi (`akm9999`), Ashutosh Agrawal (`aa12398`)

---

## ğŸ§  Project Summary

### â“ Problem Statement
The **Temporal Fusion Transformer (TFT)**, while accurate, suffers from **high latency and memory consumption**, limiting its use in real-time or edge environments.

### ğŸ’¡ Solution
We optimize TFT through:
- ğŸ”„ **FlashAttention** â€“ faster and more efficient attention computation  
- ğŸ“‰ **Quantization (PTQ)** â€“ model size compression with minimal accuracy drop  
- ğŸ›ï¸ **Hyperparameter Tuning** â€“ using Optuna to find optimal batch size and attention heads

### ğŸ¯ Value
- âš¡ **Faster Inference**
- ğŸ’¾ **Lower Memory Footprint**
- ğŸš€ **Better Deployability**

---

## ğŸ› ï¸ Tech Stack

- **Frameworks**: PyTorch, GluonTS  
- **Tools**: FlashAttention, Optuna, PyTorch Quantization  
- **Hardware**: NVIDIA A100 / V100 (NYU HPC)

---

## ğŸ§ª Experimental Pipeline

1. **Baseline Setup**: Original TFT with default config  
2. **FlashAttention Integration**: Custom kernel injection for speedup  
3. **Hyperparameter Tuning**: Grid search over batch size & heads  
4. **Quantization (PTQ)**: Compress model while retaining accuracy

---

## ğŸ“ˆ Key Results

- â±ï¸ **Training Time Reduction**:  
  - Default: `~3%`  
  - Optimal (128Bâ€“4H): `~10.9%`  

- ğŸ§  **Memory Savings**: FlashAttention reduced peak memory at larger batch sizes  

- ğŸ—œï¸ **Model Size Drop**:  
  - PTQ reduced size by **up to 74.7%**

- ğŸš¦ **Inference Time**:  
  - Large model: `1660.6s â†’ 955.9s` (**â†“ 42.4%**)  
  - Small model: `95.1s â†’ 115.3s` (**â†‘ 17%** due to dequant overhead)

---

## ğŸ“Š Observations & Conclusions

### ğŸ” Observations
- FlashAttention benefits scale with batch size
- 128 batch & 4 heads yielded optimal accuracy/performance
- Quantization slightly increased RMSE/MASE but improved speed

### âœ… Conclusions
- FlashAttention significantly improves training/inference in GPU-limited settings
- Quantized models are viable for deployment with negligible accuracy trade-off
- Post-processing & sampling, not attention, are key runtime bottlenecks

---
