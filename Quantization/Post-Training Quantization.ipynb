{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1334d077-e24b-46ea-a8e6-089ae367c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 1: Patch the mean warning and silence logging ───\n",
    "import warnings, numpy as np, logging\n",
    "from gluonts.model import forecast as _fm\n",
    "\n",
    "# Silence the specific UserWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"The mean prediction is not stored in the forecast data; the median is being returned instead\\. This behaviour may change in the future\\.\"\n",
    ")\n",
    "# Disable all WARNING and below from all loggers (including GluonTS internals)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# Override .mean on the original classes (in-place)\n",
    "def _silent_mean(self):\n",
    "    fd = getattr(self, \"_forecast_dict\", {})\n",
    "    if \"mean\" in fd:\n",
    "        return fd[\"mean\"]\n",
    "    if hasattr(self, \"samples\"):\n",
    "        return np.median(self.samples, axis=0)\n",
    "    return self.quantile(\"p50\")\n",
    "\n",
    "_fm.SampleForecast.mean   = property(_silent_mean)\n",
    "_fm.QuantileForecast.mean = property(_silent_mean)\n",
    "# ────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115659db-6ec2-45ba-8aa9-58b90c4a2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model.predictor import Predictor\n",
    "# ─── Cell 2: Core imports ───\n",
    "import pandas as pd\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from local.gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "# ─────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc07ba9-804e-4fe0-bde8-2ac7720460aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "prediction_length = 24\n",
    "context_length = 168\n",
    "window_length = context_length + prediction_length\n",
    "freq = \"1h\"\n",
    "\n",
    "def get_electricity_dataset(csv_path: str, total_samples=500_000):\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Encode categorical ID\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"categorical_id\"] = label_encoder.fit_transform(df[\"categorical_id\"].astype(str))\n",
    "\n",
    "    # Limit data to Jan 1 – Sep 1, 2014 (i.e., days_from_start < 1339)\n",
    "    full_range_df = df[df[\"days_from_start\"] < 1339]\n",
    "\n",
    "    # Sample sliding windows over full range\n",
    "    def sample_windows(subset_df):\n",
    "        samples = []\n",
    "        for entity_id, group in subset_df.groupby(\"id\"):\n",
    "            group = group.sort_values(\"date\")\n",
    "            if len(group) < window_length:\n",
    "                continue\n",
    "\n",
    "            scaler = StandardScaler().fit(group[[\"power_usage\", \"hour\", \"day_of_week\", \"t\"]].values)\n",
    "            target_scaler = StandardScaler().fit(group[[\"power_usage\"]].values)\n",
    "\n",
    "            features = scaler.transform(group[[\"power_usage\", \"hour\", \"day_of_week\", \"t\"]].values)\n",
    "            targets = target_scaler.transform(group[[\"power_usage\"]].values).flatten().astype(np.float32)\n",
    "\n",
    "            feat_hour = features[:, 1].astype(np.float32)\n",
    "            feat_dow = features[:, 2].astype(np.float32)\n",
    "            feat_time = features[:, 3].astype(np.float32)\n",
    "\n",
    "            static_cat = [group[\"categorical_id\"].iloc[0]]\n",
    "            dates = group[\"date\"].values\n",
    "\n",
    "            for i in range(0, len(group) - window_length + 1):\n",
    "                samples.append({\n",
    "                    FieldName.START: dates[i],\n",
    "                    FieldName.TARGET: targets[i:i + window_length],\n",
    "                    FieldName.FEAT_STATIC_CAT: static_cat,\n",
    "                    FieldName.FEAT_DYNAMIC_REAL: [\n",
    "                        feat_hour[i:i + window_length],\n",
    "                        feat_dow[i:i + window_length],\n",
    "                        feat_time[i:i + window_length],\n",
    "                    ],\n",
    "                })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    # Step 1: All possible windows up to Sep 1\n",
    "    all_samples = sample_windows(full_range_df)\n",
    "\n",
    "    # Step 2: Shuffle and take 500,000 total\n",
    "    np.random.shuffle(all_samples)\n",
    "    all_samples = all_samples[:total_samples]\n",
    "\n",
    "    # Step 3: Split into 450k train / 50k val\n",
    "    train_samples = all_samples[:450_000]\n",
    "    val_samples = all_samples[450_000:]\n",
    "\n",
    "    # Step 4: Test set = fixed last 7 days (same as official code)\n",
    "    test_df = df[df[\"days_from_start\"] >= 1332]\n",
    "    test_samples = sample_windows(test_df)\n",
    "\n",
    "    train_ds = ListDataset(train_samples, freq=freq)\n",
    "    val_ds = ListDataset(val_samples, freq=freq)\n",
    "    test_ds = ListDataset(test_samples, freq=freq)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, freq, prediction_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba67ac31-8dc6-4813-b14d-5db8d3c55605",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Dataset/Electricity/hourly_electricity.csv\"  # Adjust if it's in a subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb447c60-d79f-45a3-955b-fc85a55f3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 4: Load data & set precision ───\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "train_ds, val_ds, test_ds, freq, prediction_length = get_electricity_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "157063f7-cc51-42d7-9e36-9aec05770614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "orig_dir = Path(\"saved_model_flashNew\")       # your full-precision predictor folder\n",
    "int8_dir = Path(\"saved_model_flashNew_int8\")  # where to write the quantized predictor\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"       # force CPU-only\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_load_predictor(folder: Path) -> Predictor:\n",
    "    \"\"\"\n",
    "    Load a GluonTS Predictor from 'folder' on CPU, ignoring shape mismatches.\n",
    "    \"\"\"\n",
    "    import torch as _torch\n",
    "    # Monkey-patch torch.load to map to CPU and allow full unpickling\n",
    "    orig_torch_load = _torch.load\n",
    "    def cpu_load(f, **kwargs):\n",
    "        return orig_torch_load(f, map_location=\"cpu\", weights_only=False)\n",
    "    _torch.load = cpu_load\n",
    "\n",
    "    # Monkey-patch load_state_dict to ignore missing/unexpected keys\n",
    "    orig_load_state = _torch.nn.Module.load_state_dict\n",
    "    def loose_load_state(self, state_dict, strict=True):\n",
    "        return orig_load_state(self, state_dict, strict=False)\n",
    "    _torch.nn.Module.load_state_dict = loose_load_state\n",
    "\n",
    "    try:\n",
    "        pred = Predictor.deserialize(folder)\n",
    "    finally:\n",
    "        _torch.load = orig_torch_load\n",
    "        _torch.nn.Module.load_state_dict = orig_load_state\n",
    "\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d7b8c36-cafa-44b2-86c6-58b449f3adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_predictor(predictor: Predictor) -> Predictor:\n",
    "    \"\"\"\n",
    "    Apply dynamic post-training quantization to all Linear and LSTM layers\n",
    "    in the predictor's PyTorch model.\n",
    "    \"\"\"\n",
    "    model_fp = predictor.prediction_net.model.cpu()\n",
    "    quantized_model = tq.quantize_dynamic(\n",
    "        model_fp,\n",
    "        {torch.nn.Linear, torch.nn.LSTM},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    predictor.prediction_net.model = quantized_model\n",
    "    return predictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd935f43-4e36-4611-9b40-5312d61a14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictor(predictor: Predictor, test_ds, num_samples: int = 100):\n",
    "    \"\"\"\n",
    "    Run backtest + evaluation and return key metrics and timings.\n",
    "    \"\"\"\n",
    "    # Inference timing\n",
    "    t0 = time.time()\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds, predictor=predictor, num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss       = list(ts_it)\n",
    "    inf_time  = time.time() - t0\n",
    "\n",
    "    # Metrics timing\n",
    "    evaluator = Evaluator(quantiles=[0.5], num_workers=0)\n",
    "    t1 = time.time()\n",
    "    agg_metrics, _ = evaluator(iter(tss), iter(forecasts))\n",
    "    eval_time = time.time() - t1\n",
    "\n",
    "    return {\n",
    "        \"RMSE\":     agg_metrics[\"RMSE\"],\n",
    "        \"MASE\":     agg_metrics[\"MASE\"],\n",
    "        \"sMAPE\":    agg_metrics[\"sMAPE\"],\n",
    "        \"inf_time\": inf_time,\n",
    "        \"eval_time\": eval_time,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d61b19c9-17c8-44fd-8a8f-244ee0f3a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-precision size: 3.40 MB\n",
      "Quantized size:      1.07 MB\n",
      "Size reduction:      68.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def file_size_mb(path: Path) -> float:\n",
    "    return path.stat().st_size / (1024 ** 2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Prepare output folder\n",
    "# -----------------------------------------------------------------------------\n",
    "if int8_dir.exists():\n",
    "    shutil.rmtree(int8_dir)\n",
    "int8_dir.mkdir()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load full-precision predictor\n",
    "# -----------------------------------------------------------------------------\n",
    "fp_pred = safe_load_predictor(orig_dir)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Quantize a fresh copy of it\n",
    "# -----------------------------------------------------------------------------\n",
    "int8_pred = safe_load_predictor(orig_dir)\n",
    "int8_pred = quantize_predictor(int8_pred)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Serialize the quantized predictor\n",
    "# -----------------------------------------------------------------------------\n",
    "int8_pred.serialize(int8_dir)\n",
    "torch.save(\n",
    "    int8_pred.prediction_net.model.state_dict(),\n",
    "    int8_dir / \"tft_flash_weights_int8.pt\"\n",
    ")\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compare file sizes\n",
    "# -----------------------------------------------------------------------------\n",
    "orig_size = file_size_mb(orig_dir  / \"tft_flash_weights.pt\")\n",
    "int8_size = file_size_mb(int8_dir  / \"tft_flash_weights_int8.pt\")\n",
    "print(f\"Full-precision size: {orig_size:.2f} MB\")\n",
    "print(f\"Quantized size:      {int8_size:.2f} MB\")\n",
    "print(f\"Size reduction:      {(orig_size - int8_size) / orig_size * 100:.1f}%\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6957d23c-8bcb-4534-a5c0-412ae272a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 53505it [02:46, 322.28it/s]\n",
      "Running evaluation: 53505it [02:46, 321.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Evaluate both predictors on your test_ds (must be in scope)\n",
    "# -----------------------------------------------------------------------------\n",
    "metrics_fp   = evaluate_predictor(fp_pred,   test_ds)\n",
    "metrics_int8 = evaluate_predictor(int8_pred, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15e72a81-f6fe-4d65-9f55-5b1b55024d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-precision metrics: {'RMSE': np.float64(0.42567054745741684), 'MASE': np.float64(0.9383791327488374), 'sMAPE': np.float64(0.44552324287192097), 'inf_time': 99.15749335289001, 'eval_time': 166.3502960205078}\n",
      "Quantized metrics:     {'RMSE': np.float64(0.44229397355988903), 'MASE': np.float64(1.0373053124277989), 'sMAPE': np.float64(0.47795135155647406), 'inf_time': 122.00386500358582, 'eval_time': 166.88988614082336}\n",
      "Inference time ↓: -23.0%\n",
      "Eval time ↓:      -0.3%\n",
      "Inference time ↓: 99.1575s\n",
      "Inference time ↓: 122.0039s\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Print results\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Full-precision metrics:\", metrics_fp)\n",
    "print(\"Quantized metrics:    \", metrics_int8)\n",
    "print(f\"Inference time ↓: {(metrics_fp['inf_time']   - metrics_int8['inf_time'])   / metrics_fp['inf_time']   * 100:.1f}%\")\n",
    "print(f\"Eval time ↓:      {(metrics_fp['eval_time'] - metrics_int8['eval_time']) / metrics_fp['eval_time'] * 100:.1f}%\")\n",
    "print(f\"Inference time ↓: {metrics_fp['inf_time']:.4f}s\")\n",
    "# print(f\"Eval time ↓:      {(metrics_fp['eval_time'] - metrics_int8['eval_time']) / metrics_fp['eval_time'] * 100:.1f}%\")\n",
    "print(f\"Inference time ↓: {metrics_int8['inf_time']:.4f}s\")\n",
    "# print(f\"Eval time ↓:      {(metrics_fp['eval_time'] - metrics_int8['eval_time']) / metrics_fp['eval_time'] * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c49d24-0fe2-4530-bb52-79f3e885e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
